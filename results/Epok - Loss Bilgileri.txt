Epoch: 1 - Training Loss: 0.48929738338102663 - Validation Loss: 0.5854790034398307
Epoch: 2 - Training Loss: 0.44869549724981533 - Validation Loss: 0.5254961407310115
Epoch: 3 - Training Loss: 0.4174177988003921 - Validation Loss: 0.4793238530419936
Epoch: 4 - Training Loss: 0.3921474511679083 - Validation Loss: 0.4432949594436629
Epoch: 5 - Training Loss: 0.37135810212183246 - Validation Loss: 0.41473989151410634
Epoch: 6 - Training Loss: 0.3539832906885142 - Validation Loss: 0.391761178658759
Epoch: 7 - Training Loss: 0.339262283403103 - Validation Loss: 0.37301057771842083
Epoch: 8 - Training Loss: 0.32664061480982537 - Validation Loss: 0.35751908487983597
Epoch: 9 - Training Loss: 0.3157059136450707 - Validation Loss: 0.3445789181909375
Epoch: 10 - Training Loss: 0.30614582377432487 - Validation Loss: 0.3336641285697752
Epoch: 11 - Training Loss: 0.29771982395773994 - Validation Loss: 0.3243773660109339
Epoch: 12 - Training Loss: 0.29023994648632817 - Validation Loss: 0.3164138280260557
Epoch: 13 - Training Loss: 0.28355731686532887 - Validation Loss: 0.30953646303469695
Epoch: 14 - Training Loss: 0.2775525834345104 - Validation Loss: 0.3035586174490973
Epoch: 15 - Training Loss: 0.27212899871248397 - Validation Loss: 0.29833168061483234
Epoch: 16 - Training Loss: 0.2672073414901522 - Validation Loss: 0.2937361429555448
Epoch: 17 - Training Loss: 0.2627221379555424 - Validation Loss: 0.2896750254394448
Epoch: 18 - Training Loss: 0.2586188134412545 - Validation Loss: 0.28606898387185975
Epoch: 19 - Training Loss: 0.2548515201304358 - Validation Loss: 0.2828526144738876
Epoch: 20 - Training Loss: 0.2513814620440549 - Validation Loss: 0.2799716334795569
Epoch: 21 - Training Loss: 0.24817559021957786 - Validation Loss: 0.27738070104881957
Epoch: 22 - Training Loss: 0.2452055765328338 - Validation Loss: 0.27504172592944887
Epoch: 23 - Training Loss: 0.2424469994352551 - Validation Loss: 0.2729225328262132
Epoch: 24 - Training Loss: 0.23987869243047297 - Validation Loss: 0.2709958062293464
Epoch: 25 - Training Loss: 0.23748221867185051 - Validation Loss: 0.2692382469594098
Epoch: 26 - Training Loss: 0.23524144414591927 - Validation Loss: 0.2676298938168414
Epoch: 27 - Training Loss: 0.23314218854510263 - Validation Loss: 0.2661535744225968
Epoch: 28 - Training Loss: 0.23117193783218581 - Validation Loss: 0.26479445791210016
Epoch: 29 - Training Loss: 0.2293196061479286 - Validation Loss: 0.2635396884953259
Epoch: 30 - Training Loss: 0.22757533745498476 - Validation Loss: 0.26237808364316756
Epoch: 31 - Training Loss: 0.22593033938858578 - Validation Loss: 0.2612998842403491
Epoch: 32 - Training Loss: 0.22437674337087143 - Validation Loss: 0.26029654676734476
Epoch: 33 - Training Loss: 0.22290748626643186 - Validation Loss: 0.2593605696596505
Epoch: 34 - Training Loss: 0.22151620980268052 - Validation Loss: 0.258485347602723
Epoch: 35 - Training Loss: 0.2201971747169143 - Validation Loss: 0.2576650487719576
Epoch: 36 - Training Loss: 0.21894518717176817 - Validation Loss: 0.2568945110055954
Epoch: 37 - Training Loss: 0.21775553543906073 - Validation Loss: 0.25616915366838466
Epoch: 38 - Training Loss: 0.21662393521638956 - Validation Loss: 0.25548490257315115
Epoch: 39 - Training Loss: 0.21554648223218317 - Validation Loss: 0.2548381258122868
Epoch: 40 - Training Loss: 0.21451961102913028 - Validation Loss: 0.2542255787389563
Epoch: 41 - Training Loss: 0.21354005900517795 - Validation Loss: 0.25364435664951435
Epoch: 42 - Training Loss: 0.21260483494499513 - Validation Loss: 0.2530918539703212
Epoch: 43 - Training Loss: 0.21171119140021394 - Validation Loss: 0.2525657289562865
Epoch: 44 - Training Loss: 0.21085660037955747 - Validation Loss: 0.2520638730747858
Epoch: 45 - Training Loss: 0.21003873189458222 - Validation Loss: 0.2515843843845724
Epoch: 46 - Training Loss: 0.20925543497671759 - Validation Loss: 0.25112554433100814
Epoch: 47 - Training Loss: 0.20850472083933594 - Validation Loss: 0.2506857974709385
Epoch: 48 - Training Loss: 0.20778474790695087 - Validation Loss: 0.2502637337166805
Epoch: 49 - Training Loss: 0.20709380847408135 - Validation Loss: 0.24985807275172975
Epoch: 50 - Training Loss: 0.20643031679025284 - Validation Loss: 0.24946765032342336
Epoch: 51 - Training Loss: 0.2057927983961746 - Validation Loss: 0.24909140616170547
Epoch: 52 - Training Loss: 0.20517988056026634 - Validation Loss: 0.2487283733099645
Epoch: 53 - Training Loss: 0.20459028368515672 - Validation Loss: 0.2483776686848549
Epoch: 54 - Training Loss: 0.20402281357115018 - Validation Loss: 0.24803848470806966
Epoch: 55 - Training Loss: 0.20347635443848197 - Validation Loss: 0.24771008187507046
Epoch: 56 - Training Loss: 0.2029498626228338 - Validation Loss: 0.2473917821444586
Epoch: 57 - Training Loss: 0.20244236086944375 - Validation Loss: 0.24708296304750982
Epoch: 58 - Training Loss: 0.20195293316046514 - Validation Loss: 0.24678305243090937
Epoch: 59 - Training Loss: 0.20148072001826736 - Validation Loss: 0.24649152375726763
Epoch: 60 - Training Loss: 0.2010249142343116 - Validation Loss: 0.24620789189783684
Epoch: 61 - Training Loss: 0.2005847569792389 - Validation Loss: 0.245931709360329
Epoch: 62 - Training Loss: 0.2001595342550243 - Validation Loss: 0.24566256290199856
Epoch: 63 - Training Loss: 0.19974857365458087 - Validation Loss: 0.2454000704844312
Epoch: 64 - Training Loss: 0.19935124139814964 - Validation Loss: 0.24514387853185782
Epoch: 65 - Training Loss: 0.1989669396192593 - Validation Loss: 0.24489365945953012
Epoch: 66 - Training Loss: 0.19859510387606188 - Validation Loss: 0.2446491094427107
Epoch: 67 - Training Loss: 0.1982352008664945 - Validation Loss: 0.24440994640037084
Epoch: 68 - Training Loss: 0.19788672632804236 - Validation Loss: 0.24417590817074314
Epoch: 69 - Training Loss: 0.19754920310492785 - Validation Loss: 0.24394675085853637
Epoch: 70 - Training Loss: 0.19722217936735215 - Validation Loss: 0.2437222473359743
Epoch: 71 - Training Loss: 0.19690522696901122 - Validation Loss: 0.24350218588180889
Epoch: 72 - Training Loss: 0.1965979399305158 - Validation Loss: 0.24328636894429773
Epoch: 73 - Training Loss: 0.19629993303759793 - Validation Loss: 0.24307461201566322
Epoch: 74 - Training Loss: 0.1960108405440925 - Validation Loss: 0.2428667426069277
Epoch: 75 - Training Loss: 0.1957303149706728 - Validation Loss: 0.24266259931325088
Epoch: 76 - Training Loss: 0.19545802599118903 - Validation Loss: 0.24246203096093769
Epoch: 77 - Training Loss: 0.19519365939925407 - Validation Loss: 0.24226489582824687
Epoch: 78 - Training Loss: 0.19493691614840672 - Validation Loss: 0.24207106093294026
Epoch: 79 - Training Loss: 0.19468751145981883 - Validation Loss: 0.2418804013802734
Epoch: 80 - Training Loss: 0.19444517399206557 - Validation Loss: 0.24169279976576458
Epoch: 81 - Training Loss: 0.19420964506798546 - Validation Loss: 0.24150814562766237
Epoch: 82 - Training Loss: 0.19398067795410168 - Validation Loss: 0.24132633494454897
Epoch: 83 - Training Loss: 0.19375803718849158 - Validation Loss: 0.2411472696739802
Epoch: 84 - Training Loss: 0.19354149795334286 - Validation Loss: 0.24097085732844703
Epoch: 85 - Training Loss: 0.19333084548877602 - Validation Loss: 0.24079701058535194
Epoch: 86 - Training Loss: 0.1931258745448026 - Validation Loss: 0.24062564692797084
Epoch: 87 - Training Loss: 0.1929263888685603 - Validation Loss: 0.24045668831468808
Epoch: 88 - Training Loss: 0.1927322007242063 - Validation Loss: 0.24029006087405747
Epoch: 89 - Training Loss: 0.1925431304430714 - Validation Loss: 0.24012569462345312
Epoch: 90 - Training Loss: 0.19235900600187447 - Validation Loss: 0.23996352320930062
Epoch: 91 - Training Loss: 0.19217966262698352 - Validation Loss: 0.23980348366706114
Epoch: 92 - Training Loss: 0.1920049424228639 - Validation Loss: 0.23964551619932078
Epoch: 93 - Training Loss: 0.19183469402301384 - Validation Loss: 0.2394895639704536
Epoch: 94 - Training Loss: 0.19166877226181528 - Validation Loss: 0.2393355729165152
Epoch: 95 - Training Loss: 0.19150703786585696 - Validation Loss: 0.2391834915691078
Epoch: 96 - Training Loss: 0.19134935716339715 - Validation Loss: 0.23903327089208712
Epoch: 97 - Training Loss: 0.19119560181073686 - Validation Loss: 0.23888486413006796
Epoch: 98 - Training Loss: 0.19104564853436978 - Validation Loss: 0.23873822666780148
Epoch: 99 - Training Loss: 0.1908993788878591 - Validation Loss: 0.23859331589954427
Epoch: 100 - Training Loss: 0.19075667902247226 - Validation Loss: 0.2384500911076446